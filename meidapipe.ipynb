{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446db281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from keras_facenet import FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb06f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# 3D face model points\n",
    "MODEL_POINTS = np.array([\n",
    "    (0.0, 0.0, 0.0),         # Nose tip\n",
    "    (0.0, -330.0, -65.0),    # Chin\n",
    "    (-225.0, 170.0, -135.0), # Left eye left corner\n",
    "    (225.0, 170.0, -135.0),  # Right eye right corner\n",
    "    (-150.0, -150.0, -125.0),# Left mouth corner\n",
    "    (150.0, -150.0, -125.0)  # Right mouth corner\n",
    "], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316f9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDMARK_INDICES = [1, 152, 33, 263, 61, 291]  # Corresponding 2D landmarks\n",
    "def get_camera_matrix(width, height):\n",
    "    focal_length = width\n",
    "    center = (width / 2, height / 2)\n",
    "    return np.array([\n",
    "        [focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b6d8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jjona\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize FaceNet model\n",
    "facenet_model = FaceNet()\n",
    "embedding_file = \"embeddings.npz\"\n",
    "known_faces_dir = \"known_faces\"\n",
    "threshold = 0.3\n",
    "\n",
    "# Globals to hold known face embeddings and names\n",
    "known_embeddings = []\n",
    "known_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c8502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euler_angles(model_points, image_points, camera_matrix):\n",
    "    dist_coeffs = np.zeros((4, 1))  # No lens distortion\n",
    "    success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "        model_points, image_points, camera_matrix, dist_coeffs\n",
    "    )\n",
    "    if not success:\n",
    "        return None\n",
    "\n",
    "    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "    proj_matrix = np.hstack((rotation_matrix, translation_vector))\n",
    "    _, _, _, _, _, _, euler_angles = cv2.decomposeProjectionMatrix(proj_matrix)\n",
    "\n",
    "    pitch, yaw, roll = [angle[0] for angle in euler_angles]\n",
    "\n",
    "    # Normalize pitch to center around 0\n",
    "    if pitch > 90: pitch -= 180\n",
    "    elif pitch < -90: pitch += 180\n",
    "\n",
    "    return pitch, yaw, roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc836f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_direction(pitch, yaw, pitch_thresh=25, yaw_thresh=15):\n",
    "    if abs(yaw) < yaw_thresh and abs(pitch) < pitch_thresh:\n",
    "        return \"Forward\"\n",
    "    if yaw <= -yaw_thresh:\n",
    "        return \"Looking Left\"\n",
    "    if yaw >= yaw_thresh:\n",
    "        return \"Looking Right\"\n",
    "    if pitch <= -pitch_thresh:\n",
    "        return \"Looking Up\"\n",
    "    if pitch >= pitch_thresh:\n",
    "        return \"Looking Down\"\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc5c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(face_landmarks, img_w, img_h):\n",
    "    image_points = []\n",
    "    all_landmarks = []\n",
    "    for idx, lm in enumerate(face_landmarks.landmark):\n",
    "        x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "        all_landmarks.append((x, y))\n",
    "\n",
    "    for idx in LANDMARK_INDICES:\n",
    "        x, y = int(face_landmarks.landmark[idx].x * img_w), int(face_landmarks.landmark[idx].y * img_h)\n",
    "        image_points.append((x, y))\n",
    "    return np.array(image_points, dtype=np.float64), all_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92989135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_face(face):\n",
    "    os.makedirs(\"faces\", exist_ok=True)  # Create folder if not exists\n",
    "\n",
    "    # Generate a unique filename based on timestamp or count\n",
    "    import time\n",
    "    # filename = f\"faces/face_{int(time.time() * 1000)}.jpg\"\n",
    "    # cv2.imwrite(filename, face)  # Save original BGR face crop\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)  # Convert to RGB (FaceNet expects this)\n",
    "    face = cv2.resize(face, (160, 160))\n",
    "    face = face.astype(\"float32\") / 255.0\n",
    "    return np.expand_dims(face, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e11229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize FaceNet model\n",
    "facenet_model = FaceNet()\n",
    "\n",
    "# Initialize MediaPipe face detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Build known face embeddings\n",
    "def build_known_faces():\n",
    "\n",
    "    print(\"üì¶ Building embeddings for known faces...\")\n",
    "\n",
    "    for person_name in os.listdir(\"known_faces\"):\n",
    "        person_dir = os.path.join(\"known_faces\", person_name)\n",
    "        if not os.path.isdir(person_dir):\n",
    "            continue\n",
    "\n",
    "        for image_name in os.listdir(person_dir):\n",
    "            image_path = os.path.join(person_dir, image_name)\n",
    "            print(f\"üì∑ Processing: {image_path}\")\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            if image is None:\n",
    "                print(f\"‚ùå Couldn't read image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            face_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            result = face_detection.process(face_rgb)\n",
    "\n",
    "            if result.detections:\n",
    "                for detection in result.detections:\n",
    "                    bboxC = detection.location_data.relative_bounding_box\n",
    "                    h, w, _ = image.shape\n",
    "                    x = int(bboxC.xmin * w)\n",
    "                    y = int(bboxC.ymin * h)\n",
    "                    w_box = int(bboxC.width * w)\n",
    "                    h_box = int(bboxC.height * h)\n",
    "                    face_crop = image[y:y+h_box, x:x+w_box]\n",
    "\n",
    "                    if face_crop.size == 0:\n",
    "                        print(\"‚ùå Cropped face is empty, skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    face_preprocessed = preprocess_face(face_crop)\n",
    "                    embedding = facenet_model.model.predict(face_preprocessed)[0]  # ‚¨ÖÔ∏è FIXED LINE\n",
    "                    known_embeddings.append(embedding)\n",
    "                    known_names.append(person_name)\n",
    "                    print(f\"‚úÖ Added {person_name}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Total faces embedded: {len(known_names)}\")\n",
    "    np.savez(\"embeddings.npz\", embeddings=known_embeddings, names=known_names)\n",
    "    return known_embeddings, known_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce26a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_known_faces():\n",
    "    global known_embeddings, known_names  # ‚úÖ Add this line\n",
    "\n",
    "    if os.path.exists(embedding_file):\n",
    "        print(\"Loading embeddings from file...\")\n",
    "        data = np.load(embedding_file, allow_pickle=True)\n",
    "        known_embeddings = data[\"embeddings\"]\n",
    "        known_names = data[\"names\"].tolist()\n",
    "    else:\n",
    "        known_embeddings, known_names = build_known_faces()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b36a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def recognize_face(embedding, position):\n",
    "    min_dist = float(\"inf\")\n",
    "    identity = \"Unknown\"\n",
    "\n",
    "    for idx, known_embedding in enumerate(known_embeddings):\n",
    "        dist = cosine(embedding, known_embedding)  # Lower = more similar\n",
    "        # print(f\"Distance to {known_names[idx]}: {dist:.4f}\")\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = known_names[idx] + '(' + position + ')'\n",
    "\n",
    "    if min_dist > threshold:\n",
    "        identity = \"Unknown\"\n",
    "\n",
    "    return identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9669569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_annotations(frame, direction, position, all_landmarks, yaw, pitch):\n",
    "    xs = [x for x, y in all_landmarks]\n",
    "    ys = [y for x, y in all_landmarks]\n",
    "    x_min, x_max = min(xs), max(xs)\n",
    "    y_min, y_max = min(ys), max(ys)\n",
    "\n",
    "    # label = f\"{direction}, y:{int(abs(yaw)):.2f}, p:{int(abs(pitch)):.2f}\"\n",
    "    # cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    # cv2.putText(frame, label, (x_min, y_min - 10),\n",
    "    #             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    # Only recognize if the person is looking forward\n",
    "    if direction == \"Forward\":\n",
    "        face_crop = frame[y_min:y_max, x_min:x_max]\n",
    "        if face_crop.size > 0:\n",
    "            face_preprocessed = preprocess_face(face_crop)\n",
    "            face_embedding = facenet_model.model.predict(face_preprocessed)[0]\n",
    "            identity = recognize_face(face_embedding, position)\n",
    "        else:\n",
    "            identity = \"Face not found\"\n",
    "    else:\n",
    "        identity = \"Not looking\"\n",
    "    \n",
    "    # Draw annotations\n",
    "    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    label = f\"{identity}\"\n",
    "    cv2.putText(frame, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c115390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clock_direction(x, img_width):\n",
    "    relative_pos = x / img_width\n",
    "\n",
    "    if relative_pos < 0.15:\n",
    "        return \"9 o'clock\"\n",
    "    elif relative_pos < 0.30:\n",
    "        return \"10 o'clock\"\n",
    "    elif relative_pos < 0.45:\n",
    "        return \"11 o'clock\"\n",
    "    elif relative_pos < 0.55:\n",
    "        return \"12 o'clock\"\n",
    "    elif relative_pos < 0.70:\n",
    "        return \"1 o'clock\"\n",
    "    elif relative_pos < 0.85:\n",
    "        return \"2 o'clock\"\n",
    "    else:\n",
    "        return \"3 o'clock\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8928654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_center_x(landmarks):\n",
    "    xs = [pt[0] for pt in landmarks]\n",
    "    return sum(xs) / len(xs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6cf11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6d32d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from file...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    load_known_faces()\n",
    "    facenet_model = FaceNet()\n",
    "    cap = cv2.VideoCapture('./test3.mp4')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30  # Fallback in case of 0\n",
    "    delay = int(1000 / fps)\n",
    "    # delay = 1\n",
    "    \n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=5,\n",
    "                               min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            img_h, img_w = frame.shape[:2]\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(rgb_frame)\n",
    "\n",
    "            if results.multi_face_landmarks:\n",
    "                for face_landmarks in results.multi_face_landmarks:\n",
    "                    image_points, all_landmarks = extract_landmarks(face_landmarks, img_w, img_h)\n",
    "                    camera_matrix = get_camera_matrix(img_w, img_h)\n",
    "                    angles = get_euler_angles(MODEL_POINTS, image_points, camera_matrix)\n",
    "                    if angles is None:\n",
    "                        continue\n",
    "                    pitch, yaw, roll = angles\n",
    "                    direction = classify_direction(pitch, yaw)\n",
    "                    center_x = get_face_center_x(all_landmarks)\n",
    "                    position = get_clock_direction(center_x, img_w)\n",
    "                    draw_annotations(frame, direction, position, all_landmarks, yaw, pitch)\n",
    "\n",
    "            scale_factor = 0.5\n",
    "            display_frame = cv2.resize(frame, (int(img_w * scale_factor), int(img_h * scale_factor)))\n",
    "            cv2.imshow(\"Video\", display_frame)\n",
    "\n",
    "            if cv2.waitKey(delay) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff261226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af3b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a8a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cbd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22958e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input1 sum: 27632.178\n",
      "Input2 sum: 47494.445\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Embedding 1 first 10: [ 0.01105747  0.02691554 -0.01947343 -0.01546158 -0.03590532 -0.00074519\n",
      "  0.00959834  0.03275811  0.03332273 -0.0075451 ]\n",
      "Embedding 2 first 10: [ 0.06260297 -0.01816018  0.05560359 -0.06730364  0.06026611  0.06765513\n",
      " -0.01969156  0.0125491   0.00032403  0.0750424 ]\n",
      "Distance between embeddings: 1.4861053\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras_facenet import FaceNet\n",
    "\n",
    "def preprocess_face(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "    face = cv2.resize(face, (160, 160))\n",
    "    face = face.astype(\"float32\") / 255.0\n",
    "    return np.expand_dims(face, axis=0)\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread(\"known_faces/nahin/1.jpg\")\n",
    "img2 = cv2.imread(\"known_faces/elon/1.jpg\")\n",
    "\n",
    "input1 = preprocess_face(img1)\n",
    "input2 = preprocess_face(img2)\n",
    "\n",
    "print(\"Input1 sum:\", np.sum(input1))\n",
    "print(\"Input2 sum:\", np.sum(input2))\n",
    "\n",
    "# Initialize model once\n",
    "facenet_model = FaceNet()\n",
    "\n",
    "# Use raw model.predict() instead of .embeddings() to avoid caching issues\n",
    "emb1 = facenet_model.model.predict(input1)[0]\n",
    "emb2 = facenet_model.model.predict(input2)[0]\n",
    "\n",
    "print(\"Embedding 1 first 10:\", emb1[:10])\n",
    "print(\"Embedding 2 first 10:\", emb2[:10])\n",
    "print(\"Distance between embeddings:\", np.linalg.norm(emb1 - emb2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e554939",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ 0.01040817  0.01323742 -0.08136161  0.03703693 -0.00583532  0.09873082\n",
    " -0.02493108  0.06667083 -0.03647001 -0.02634569 -0.01645712  0.07746425\n",
    "  0.02978942 -0.03657433 -0.0085121  -0.02543744  0.01909423  0.03340082\n",
    " -0.01135216 -0.12970872 -0.09350711 -0.00186395  0.07484037 -0.04299356\n",
    "  0.03251214 -0.01524376 -0.01608279 -0.04873962 -0.04360052  0.02108771\n",
    " -0.00943475 -0.00025414  0.00208789 -0.02367694 -0.03761331  0.06066559\n",
    "  0.04307509 -0.01791337 -0.11657034  0.02633476  0.01289774  0.03899858\n",
    " -0.00975658 -0.02284099  0.03940051  0.00723536  0.03981997  0.08939075\n",
    " -0.10278585 -0.0819013 ]\n",
    "\n",
    "[ 0.01040817  0.01323742 -0.08136161  0.03703693 -0.00583532  0.09873082\n",
    " -0.02493108  0.06667083 -0.03647001 -0.02634569 -0.01645712  0.07746425\n",
    "  0.02978942 -0.03657433 -0.0085121  -0.02543744  0.01909423  0.03340082\n",
    " -0.01135216 -0.12970872 -0.09350711 -0.00186395  0.07484037 -0.04299356\n",
    "  0.03251214 -0.01524376 -0.01608279 -0.04873962 -0.04360052  0.02108771\n",
    " -0.00943475 -0.00025414  0.00208789 -0.02367694 -0.03761331  0.06066559\n",
    "  0.04307509 -0.01791337 -0.11657034  0.02633476  0.01289774  0.03899858\n",
    " -0.00975658 -0.02284099  0.03940051  0.00723536  0.03981997  0.08939075\n",
    " -0.10278585 -0.0819013 ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
